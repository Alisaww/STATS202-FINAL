---
title: "Untitled"
author: "Alisaww"
date: "2022-07-30"
output: html_document
---
```{r}
knitr::opts_chunk$set(echo = TRUE)
```

Gradient Boosting (h2o)
```{r}
h2o.no_progress()
h2o.init(max_mem_size = "16g") # have 16g ram total
```


```{r}
# feature names
y <- "PANSS_Total"
x <- setdiff(names(training[ ,-"PatientID"]), y)
# turn training set into h2o object
train.h2o <- as.h2o(training[ ,-"PatientID"])
# GBM model with defaults
h2o.fit2 <- h2o.gbm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 10,
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  max_runtime_secs = 60*10,
  seed = 1
)
# model stopped after xx trees
h2o.fit2@parameters$ntrees

# cross validated MSE
h2o.rmse(h2o.fit2, xval = TRUE)^2
# assess model results
h2o.fit2
```

Full grid 
```{r}
# create training & validation sets
index <- h2o.splitFrame(train.h2o, ratios = 0.75)
train <- index[[1]]
valid <- index[[2]]
# create hyperparameter grid
rm(hyper_grid)
hyper_grid = list(
  max_depth = c(1,3,4), # depth of each tree
  min_rows = c(5,10,20), # minimum observations in a terminal node
  learn_rate = c(0.005, 0.01, 0.05, 0.1),
  learn_rate_annealing = c(1), # 1 tends to always beat 0.99
  sample_rate = c(.65, 0.7, 0.75, 0.8), # row sample rate. better to have less than 1 it seems
  col_sample_rate = c(0.7, .8, .9) # always better to have less than 1 here
)
# combinations
nrow(expand.grid(hyper_grid))
```

Random discrete grid search
```{r}
# random grid search 
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.005, # MSE tolerance
  stopping_rounds = 10,   # if 10 consecutive trees have no improvement it stops 
  max_runtime_secs = 60*1 # limit debugging time 
  )
# perform grid search
gbm_grid2 <- h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid2",
  x = x,
  y = y,
  training_frame = train,
  validation_frame = valid,
  hyper_params = hyper_grid,
  search_criteria = search_criteria, # add search criteria
  ntrees = 10000,
  #stopping_rounds = 10, # stop if none of the last 10 models managed to have a 0.5% improvement in MSE compared to best model before that
  #stopping_tolerance = 0,
  seed = 1
  )
# collect the results and sort by our model performance metric of choice
grid_perf <- h2o.getGrid(
  grid_id = "gbm_grid2",
  sort_by = "mse",
  decreasing = FALSE
  )
grid_perf
```

Top 5 models all have `max_depth` of 5 when the options are 1, 3, and 5.
Same if the options are 1, 5, 10.
Chooses 4 if options are 3,4,5,6,7 so let's keep it below 4. 
If I set max_depth to 1,2,3,4 it still always chooses 4 and sometimes 3. 
They also always have `learn_rate_annealing` of 1. 
Best column sample rates are never 1, always 0.8 or 0.9. 
Best models tend to use lowest learning rate of 0.01 so far. 
All use at least min_rows of 5.  
All have sample rate of at least 0.65.

```{r h20-performance}
# Use validation error to grab the model_id for the top model
best_model_id <- grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
#get performance metrics on the best model
h2o.performance(model = best_model, valid = TRUE)
```

```{r}
# train final model
h2o.final <- h2o.gbm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 10,
  ntrees = 10000,
  learn_rate = 0.1,
  learn_rate_annealing = 1,
  max_depth = 4,
  min_rows = 10,
  sample_rate = 0.75,
  col_sample_rate = 0.9,
  stopping_rounds = 10,
  #stopping_tolerance = 0.005,
  seed = 1
)
# model stopped after xx trees
h2o.final@parameters$ntrees
# cross validated MSE
h2o.rmse(h2o.final, xval = TRUE)^2
```

Visualization
```{r}
# model stopped after xx trees
h2o.final@parameters$ntrees
# cross validated MSE
h2o.rmse(h2o.final, xval = TRUE)^2
h2o.varimp_plot(h2o.final, num_of_features = 5)
```

Prediction
```{r}
# convert test set to h2o object
h2o_test <- as.h2o(select_patients)
# evaluate on new data
h2o.performance(model = h2o.final, newdata = h2o_test)
# predict values
h2o.predict(h2o.final, newdata = h2o_test) # predict with h2o.predict
h2o_test$prediction = predict(h2o.final, h2o_test) # gives same result as above
# write to csv 
h2o_forecast <- as.h2o(test)
h2o_forecast$PANSS_Total = predict(h2o.final, h2o_forecast)
h2o.exportFile(h2o_forecast[,c("PatientID","PANSS_Total")],'h2o-predict.csv',force=TRUE)
```

Random Forests
```{r}
library(randomForest) 
set.seed(1)
# default RF model
RF <- randomForest(
  formula = PANSS_Total ~ .,
  data    = training[,-"PatientID"],
  mtry = 2
)
RF
plot(RF)
```

```{r}
# number of trees with lowest MSE
which.min(RF$mse)
# MSE of this optimal random forest
m1$mse[which.min(RF$mse)]
```

```{r}
require(tidyr)
require(dplyr)
set.seed(1)
# split data
training1 = sample(1:nrow(training), floor(nrow(training)*0.8))
train_v2 = training[training1,]
valid = training[-training1,]
x_test = valid
y_test = valid$PANSS_Total

 rf <- randomForest(
  formula = PANSS_Total ~ .,
  data    = train_v2[,-"PatientID"],
  xtest   = x_test[,-c("PatientID","PANSS_Total")],
  ytest   = y_test
)
# extract OOB & validation errors
oob <- rf$mse
validation <- rf$test$mse
# compare error rates
tibble::tibble(
  `Out of Bag Error` = oob,
  `Test error` = validation,
  ntrees = 1:rf $ntree
) %>%
  gather(Metric, MSE, -ntrees) %>%
  ggplot(aes(ntrees, MSE, color = Metric)) +
  geom_line() +
  xlab("Number of trees")
```

Tuning via h2o
```{r}
h2o.init(max_mem_size = "6g")
set.seed(1)
# create feature names
y <- "PANSS_Total"
x <- setdiff(names(training[,-"PatientID"]), y)
# turn training set into h2o object
train.h2o <- as.h2o(training[,-"PatientID"])
# second hypergrid
hyper_grid.h2o <- list(
  ntrees      = seq(300, 550, by = 50),
  mtries      = 2,
  max_depth   = seq(15, 45, by = 5),
  min_rows    = seq(7, 11, by = 1),
  nbins       = seq(5, 25, by = 5),
  sample_rate = c(0.4,0.45,0.5,0.55,.6,.65,.7)
)
# random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 60*1  
  )
# build grid search
random_grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid2",
  x = x,
  y = y,
  training_frame = train.h2o,
  hyper_params = hyper_grid.h2o,
  search_criteria = search_criteria
  )
grid_perf2 <- h2o.getGrid(
  grid_id = "rf_grid2",
  sort_by = "mse",
  decreasing = FALSE
  )
print(grid_perf2)
```

```{r}
# first grid
hyper_grid.h2o <- list(
  ntrees      = seq(200, 500, by = 150), # best all had 350 min so set 350 as new min
  mtries      = seq(2,4, by = 1), # best all have 2 so set this identically to 2
  max_depth   = seq(20, 40, by = 5),
  min_rows    = seq(1, 5, by = 2), # best all have 5 (so set 5 as min)
  nbins       = seq(10, 30, by = 5),
  sample_rate = c(.55, .632, .75) # best all have 0.55 so vary around this
) 
# second hypergrid
hyper_grid.h2o <- list(
  ntrees      = seq(350, 500, by = 75), # none of the top 5 use 500
  mtries      = 2,
  max_depth   = seq(20, 40, by = 5), # none of top 5 use 40
  min_rows    = seq(5, 10, by = 2), # none of the top 5 models use 5
  nbins       = seq(10, 30, by = 5), # none of top 5 use 10
  sample_rate = c(0.45,.55, .65) # none of the top 5 models use sample_rate of 0.65
)
```


```{r}
# Grab the model_id for the top model
best_model_id <- grid_perf2@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
h2o.varimp_plot(best_model)
# Now let’s evaluate the model performance on a test set
select_patients_df.h2o <- as.h2o(select_patients_df)
best_model_perf <- h2o.performance(model = best_model, newdata = select_patients_df.h2o)
# View prediction
prediction = predict(best_model, select_patients_df.h2o)
plot(as.vector(prediction), select_patients_df$PANSS_Total,xlim=c(30,100), ylim=c(30,100))
abline(0,1) # line with y-intercept 0 and slope 1
# RMSE of best model
h2o.mse(best_model_perf)
# write to csv for Kaggle submission
forecast.h2o <- as.h2o(test_df)
forecast.h2o$PANSS_Total = predict(best_model, forecast.h2o)
h2o.exportFile(forecast.h2o[,c("PatientID","PANSS_Total")],'rf-prediction.csv',force=TRUE)
```



Linear models
```{r}
training = subset(training, Study=="E", select = c(PatientID, TxGroup, VisitDay, Study, PANSS_Total))
training_df = subset(training, select = c(PatientID, TxGroup, VisitDay, PANSS_Total))
MSE = subset(MSE, Study=="E", select = c(PatientID, TxGroup, VisitDay, Study, PANSS_Total))
MSE = subset(MSE, select = c(PatientID, TxGroup, VisitDay, PANSS_Total))
```

Linear regression
Fit a linear model using least squares on the training set, and report the test error obtained.
```{r linear}
linear = lm(PANSS_Total ~., data=training_df)
summary(linear)
# Calculate test MSE
mean((MSE$PANSS_Total - predict(linear, MSE))^2)
```

Ridge regression
Fit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained.
```{r}
library(glmnet)
set.seed(1)
# Create design matrices
train.mat = model.matrix(PANSS_Total ~ .-PatientID, data = training_df)
test.mat = model.matrix(PANSS_Total ~ .-PatientID, data = MSE)
#grid=10^seq(10,-3,length=100)
#ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=0,lambda=grid, thresh=1e-12)
ridge.mod=glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge.mod, xvar = "lambda")
# Find lambda through cross-validation
cv.out = cv.glmnet(train.mat, training_df$PANSS_Total, alpha = 0)
ridge.cv.out = cv.out
plot(cv.out)
best = cv.out$lambda.min
best
# Calculate test MSE
ridge.pred=predict(ridge.mod,s=best,newx=test.mat)
mean((ridge.pred - MSE$PANSS_Total)^2)
```

ridge-lambda
The first and second vertical dashed lines represent the λ value with the minimum MSE and the largest λ value within one standard error of the minimum MSE. 
```{r}
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
# visualize how much we can restrain coefficients while still having predictive accuracy
ridge_min <- glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge_min, xvar = "lambda")
abline(v = log(cv.out$lambda.1se), col = "blue", lty = "dashed")
library(broom)
coef(cv.out, s = "lambda.1se") %>%
  tidy() %>%
  filter(row != "(Intercept)") %>%
  top_n(25, wt = abs(value)) %>%
  ggplot(aes(value, reorder(row, value))) +
  geom_point() +
  ggtitle("Top 25 influential variables") +
  xlab("Coefficient") +
  ylab(NULL)
```

Lasso regression
Fit a lasso model on the training set, with λ chosen by crossvalidation
```{r}
library(glmnet)
set.seed(1)
lasso.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1)
plot(lasso.mod, xvar = "lambda")
# Find lambda through cross-validation
cv.out=cv.glmnet(train.mat, training_df$PANSS_Total,alpha=1)
lasso.cv.out = cv.out
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
# Calculate test MSE
lasso.pred=predict(lasso.mod,s=bestlam,newx=test.mat)
mean((lasso.pred-MSE$PANSS_Total)^2)
predict(cv.out, s = bestlam, type = "coefficients")
```

```{r}
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
# visualize lasso results
lasso.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1)
plot(lasso.mod, xvar = "lambda")
abline(v = log(cv.out$lambda.min), col = "red", lty = "dashed")
abline(v = log(cv.out$lambda.1se), col = "red", lty = "dashed")
# most influential variables
coef(cv.out, s = "lambda.1se") %>%
  tidy() %>%
  filter(row != "(Intercept)") %>%
  ggplot(aes(value, reorder(row, value), color = value > 0)) +
  geom_point(show.legend = FALSE) +
  ggtitle("Influential variables") +
  xlab("Coefficient") +
  ylab(NULL)
```

```{r}
# minimum Ridge MSE
min(ridge.cv.out$cvm)
plot(ridge.pred,MSE$PANSS_Total,xlim=c(20,100), ylim=c(20,100))
# minimum Lasso MSE
min(lasso.cv.out$cvm)
plot(lasso.pred,MSE$PANSS_Total,xlim=c(20,100), ylim=c(20,100))
```

Multivariate Adaptive Regression Spline (MARS)
```{r}
library(earth)     
# basic MARS model
mars1 <- earth(
  PANSS_Total ~ .,
  data = training_df[,-"PatientID"]
)
# Print model summary
print(mars1)
summary(mars1) %>% .$coefficients %>% head(10)
plot(mars1, which = 1)
```

```{r}
# Fit a basic MARS model
mars2 <- earth(
  PANSS_Total ~ .,
  data = training_df[,-"PatientID"],
  degree = 3
)
# check out the first 10 coefficient terms
print(mars2)
summary(mars2) %>% .$coefficients %>% head(10)
plot(mars2, which = 1)
```

Tuning
```{r}
# create a tuning grid
hyper_grid <- expand.grid(
  degree = 1:3,
  nprune = seq(1, 16, by = 2)
  )
head(hyper_grid)
```

```{r}
library(caret)
set.seed(1)
# cross validated model
tuned_mars <- train(
  x = subset(training_df[,-"PatientID"], select = -PANSS_Total),
  y = training_df$PANSS_Total,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid
)
# best model
tuned_mars$bestTune
summary(tuned_mars)
# plot results
ggplot(tuned_mars)
```


Visualize
```{r}
library(vip)       # variable importance
# variable importance plots
p1 <- vip(tuned_mars, num_features = 40, bar = FALSE, value = "gcv") + ggtitle("GCV")
p2 <- vip(tuned_mars, num_features = 40, bar = FALSE, value = "rss") + ggtitle("RSS")
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

```{r}
test_predict = predict(tuned_mars, MSE)
mean((MSE$PANSS_Total - test_predict)^2)
plot(as.vector(test_predict), MSE$PANSS_Total,xlim=c(30,100), ylim=c(30,100))
abline(0,1)
test$PANSS_Total = predict(tuned_mars, test)
write.csv(test[,c("PatientID","PANSS_Total")],'mars-forecast.csv',row.names=FALSE)
```
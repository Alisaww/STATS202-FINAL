---
title: "Classification"
author: "Runnan Dai"
date: "07/08/2022"
output: html_document
---
### Setup
```{r}
rm(list = ls()) 
library(dplyr)
library(ggplot2)
```

```{r}
library(readr)
StudyA=read_csv("Study_A.csv")
StudyB=read_csv("Study_B.csv")
StudyC=read_csv("Study_C.csv")
StudyD=read_csv("Study_D.csv")
StudyE=read_csv("Study_E.csv")
summary(StudyE)
names(StudyE)
dim(StudyE)[1]
```

The assessments we should take into account for the Kaggle submission are listed below. This ought to correspond to Study E.

```{r}
samp_subm = read.csv("sample_submission_status.csv")
prediction.ids = samp_subm$AssessmentID 
#The AssessmentIDs will be used for Kaggle submission
length(unique(prediction.ids))
length(prediction.ids)
all(StudyE$AssessmentID==prediction.ids)
```

This is the same length as Study E. All the values are in both.

### Data reorganizing

Remove columns which not corresponding to our predictors and response.

```{r}
A = subset(StudyA, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
B = subset(StudyB, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
C = subset(StudyC, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
D = subset(StudyD, select = c(Country,TxGroup,VisitDay,PANSS_Total,LeadStatus))
names(StudyE)
E = subset(StudyE, select = c(Country,TxGroup,VisitDay,PANSS_Total,AssessmentID))
names(E)
```

Combine the subset dataframes into one dataframe.

```{r}
combABCD = rbind(A,B,C,D)
summary(combABCD)
names(combABCD)
```

Eliminate any overlaps using AssessmentID.

```{r}
# Before
dim(combABCD)[1]
# After
combABCD=distinct(combABCD)
dim(combABCD)[1]
```

In addition, ID numbers must be factors rather than numerical information. In reality, everything should be factorized except for VisitDay and PANSS Total.

```{r}
combABCD <- mutate_at(combABCD, vars(Country, TxGroup, LeadStatus), as.factor)
str(combABCD) 
#Clearly depict the object's structure
E<-mutate_at(E,vars(Country, TxGroup, AssessmentID),as.factor)
str(E)
```

Turn LeadStatus into a binary categorization of Passed or Flagged since we just want the probability of flagged OR allocated to CS.

```{r}
combABCD$LeadStatus[combABCD$LeadStatus!="Passed"]<-"Flagged"
combABCD$LeadStatus=factor(combABCD$LeadStatus)
table(combABCD$LeadStatus)
```

Create training, development, and test sets from the data. Here, a test set is Study E, to which we have no response at all.

```{r}
set.seed(100)
# Total number of observations
total=1:dim(combABCD)[1] 
# Let 75% of observations into training set
train=sample(total,length(total)*0.75)
combABCD.train=combABCD[train,]
head(combABCD.train)
# Rest will be the development set
dev=total[-train]
combABCD.dev=combABCD[dev,]
# Study E is the test set
test=E
```

### Naive Bayes

I test the Naive Bayes Classifier first. To perform this, I must rely on the independent nature of each predictor (PatientID, Country, TxGroup, VisitDay, Study,PANSS Total, SiteID, RaterID, AssessmentID). Since Naive Bayes assumes a normal distribution for all quantitative variables, we must also remove VisitDay as a predictor. In order to forecast on Study E, we must eliminate any IDs that are absent from Study E. (the IDs unique to other studies)

```{r}
library(dplyr)
library(h2o)
library(caret)
library(corrplot)
Y.train="LeadStatus"
X.train=setdiff(names(combABCD.train),c(Y.train,"VisitDay"))

# h2o.no_progress()
h2o.init()

combABCD.train.h2o <- combABCD.train %>%
  mutate_if(is.factor, factor, ordered = FALSE) %>%
  as.h2o()
str(combABCD.train.h2o)
train.naiveB <- h2o.naiveBayes(
  x = X.train,
  y = Y.train,
  training_frame = combABCD.train.h2o,
  nfolds = 10,
  laplace = 0
)

# Analyze training data results
cM.naiveB=h2o.confusionMatrix(train.naiveB)
accuracy.naiveB=(cM.naiveB[1,1]+cM.naiveB[2,2])/(cM.naiveB[3,1]+cM.naiveB[3,2])
print(cM.naiveB)
print(paste("Training accuracy: =",accuracy.naiveB))


# ROC curve on the development data
names(combABCD.dev)
# Remove VisitDay
combABCD.dev.h2o=combABCD.dev[,-3]
names(combABCD.dev.h2o)
combABCD.dev.h2o=combABCD.dev.h2o %>%
  mutate_if(is.factor,factor,ordered=FALSE) %>%
  as.h2o()

performance.train=h2o.performance(train.naiveB,xval=TRUE)
performance.dev=h2o.performance(train.naiveB,newdata=combABCD.dev.h2o)

logloss.train = h2o.logloss(performance.train,xval=TRUE)
logloss.dev=h2o.logloss(performance.dev,xval=TRUE)
auc.train <- h2o.auc(performance.train,xval=TRUE)
auc.dev <- h2o.auc(performance.dev)
fpr.dev <- h2o.fpr(performance.dev) %>% .[['fpr']]
tpr.dev <- h2o.tpr(performance.dev) %>% .[['tpr']]
data.frame(fpr = fpr.dev, tpr = tpr.dev) %>%
  ggplot(aes(fpr, tpr) ) +
  geom_line() +
  ggtitle( sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) ) +
  theme_minimal()
ggsave("Predictions/naive_bayes_roc.png",width=6,height=4,units="in",device="png",dpi="print")

# Predict values with predict -->
names(test)
test.h2o=test[,-3]#get rid of VisitDay
#test.h2o=test.h2o[,-4]#get rid of AssessmentiD
test.h2o=test.h2o %>%
  mutate_if(is.factor,factor,ordered=FALSE)%>%
  as.h2o()
str(test.h2o)

naiveB.predictions=h2o.predict(train.naiveB,test.h2o)
naiveB.predictions_df=as.data.frame(naiveB.predictions)
test$LeadStatus=naiveB.predictions_df$Flagged
test.output=test[,c("AssessmentID","LeadStatus")]
write.csv(test.output,"Predictions/test.nb.csv",row.names=FALSE)

```

The holdout test data don't perform well for the naive classifier. Although the overall development AUC is $0.7698$, there are high false and true positives rates.

### Logistic

### Training - validation split

A logistic regression may be tried. I remove the country as a predictor because Study E does not have the UK as a country.

```{r}
library(pROC)
names(combABCD.train)
# Exclude country from being a predictor
combABCD.train.glm = combABCD.train[,-1]
attach(combABCD.train.glm)
names(combABCD.train.glm)
train.glm = glm(LeadStatus~.,data = combABCD.train.glm,family = binomial)
summary(train.glm)
# Check dummy encoding for Up/Down
contrasts(LeadStatus)

# Development test
glm.probs.flag.dev = 1-predict(train.glm,combABCD.dev,type = "response")
# Generate a predictions vector with the same length as the development dataset.
glm.pred = rep("Passed",dim(combABCD.dev)[1])
# Change relevant values to "Flagged" based on model-predicted value.
glm.pred[glm.probs.flag.dev>0.2] = "Flagged"
table(glm.pred,combABCD.dev$LeadStatus)
roc.dev = roc(LeadStatus~glm.probs.flag.dev,data = combABCD.dev)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))


# Kaggle test
test = E
# Make predictions based on each observation's fit; 1 = Passed.
glm.probs = predict(train.glm,test,type = "response")
# Prob of being flagged for all.
glm.probs.flag = 1-glm.probs
names(test)
test.output.glm = as.data.frame(test$AssessmentID)
test.output.glm$LeadStatus = glm.probs.flag
colnames(test.output.glm)[colnames(test.output.glm)=="AssessmentID"] <- "AssessmentID"
write.csv(test.output.glm,"Predictions/test.glm.csv",row.names = FALSE)
ggsave("Figures/glm_roc.png",width=6,height=4,units="in",device="png",dpi="print")
```

It appears that statistically significant determinants of Passed vs. Flagged include PANSS Total, VisitDay, Study, TxGroup, and various countries.

```{r}
plot(test.output.glm$LeadStatus,naiveB.predictions_df$Flagged,xlim = c(0,0.5),ylim = c(0,0.5))
abline(0,1)
```

```{r}
library(ROCR)
# AUC
auc.dev = prediction(predict(train.glm,combABCD.dev,type = "response"), combABCD.dev$LeadStatus) %>%
  performance(measure = "auc") %>%
  .@y.values

# Cross-entropy
logloss.dev = prediction(predict(train.glm,combABCD.dev,type = "response"), ifelse(combABCD.dev$LeadStatus=="Passed", 1, 0)) %>%
  performance(measure = "mxe") %>%
  .@y.values

# ROC curves
prediction(predict(train.glm,combABCD.dev,type = "response"), combABCD.dev$LeadStatus) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot(main = sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) )
```

### Full training set

```{r}
full.glm = glm(LeadStatus~.,data = combABCD[,-1],family = binomial)
summary(full.glm)
# Check dummy encoding for Up/Down
contrasts(LeadStatus)
```

```{r}
test = E
# Make predictions based on each observation's fit; 1 = Passed.
glm.probs = predict(full.glm,test,type = "response") 
# Prob of being flagged for all.
glm.probs.flag = 1 - glm.probs 
test.full.glm = as.data.frame(test$AssessmentID)
test.full.glm$LeadStatus = glm.probs.flag
colnames(test.full.glm)[colnames(test.full.glm)=="AssessmentID"] <- "AssessmentID"
write.csv(test.full.glm,"Predictions/test.glm.full.csv",row.names = FALSE)
```

```{r}
plot(test.output.glm$LeadStatus,test.full.glm$LeadStatus,xlim = c(0,0.5),ylim = c(0,0.5))
abline(0,1)
```

### Logistic regression (all individual scores)

```{r}
# Create dataframe that has all individual PANSS scores
combABCD.all = rbind(StudyA, StudyB, StudyC, StudyD)
combABCD.all = subset(combABCD.all,select = setdiff(names(combABCD.all),c("Country","Study","PatientID","RaterID","AssessmentID","PANSS_Total","SiteID")))
names(combABCD.all)
combABCD.all = distinct(combABCD.all)
# Cohesively display the object's structure
combABCD.all <- mutate_at(combABCD.all, vars(TxGroup,LeadStatus), as.factor)
str(combABCD.all) 

# Fix up LeadStatus column for the purposes of part 4
combABCD.all$LeadStatus[combABCD.all$LeadStatus!="Passed"]<-"Flagged"
combABCD.all$LeadStatus = factor(combABCD.all$LeadStatus)
# See how many passed and not
table(combABCD.all$LeadStatus)

# Split into training and development set
set.seed(100)
# Total number of observations
total = 1:dim(combABCD.all)[1]
# Put 75% of observations into training set
train = sample(total,length(total)*0.75) 
combABCD.train.all = combABCD.all[train,]
head(combABCD.train.all)
# Rest go into development set
dev = total[-train] 
combABCD.dev.all = combABCD.all[dev,]

# Create test set(E) that has all individual scores
E_tmp = subset(StudyE,select = setdiff(names(StudyE),c("Country","Study","PatientID","RaterID","PANSS_Total","SiteID")))
E_tmp = mutate_at(E_tmp,vars(TxGroup,AssessmentID),as.factor)
test.all = E_tmp 
```

```{r}
train.all.glm <- glm(LeadStatus ~., family = "binomial", data = combABCD.train.all)
summary(train.all.glm)
```

```{r}
library(broom)
tidy(train.all.glm)
caret::varImp(train.all.glm)

# Development test
glm.probs.flag.dev = 1 - predict(train.all.glm, combABCD.dev.all, type = "response")
glm.pred = rep("Passed", dim(combABCD.dev.all)[1])
glm.pred[glm.probs.flag.dev > 0.2] = "Flagged"
table(glm.pred,combABCD.dev.all$LeadStatus)
roc.dev = roc(LeadStatus~glm.probs.flag.dev,data = combABCD.dev.all)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))

# Kaggle test
test = test.all
glm.probs = predict(train.all.glm, test, type = "response")
glm.probs.flag = 1 - glm.probs
names(test)
test.all.glm = as.data.frame(test$AssessmentID)
test.all.glm$LeadStatus = glm.probs.flag
colnames(test.all.glm)[colnames(test.all.glm)=="test$AssessmentID"] <- "AssessmentID"
write.csv(test.all.glm,"Predictions/test.all.glm.csv",row.names = FALSE)

# Compare to original logistic regression
plot(test.output.glm$LeadStatus,test.all.glm$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5),xlab = "Logistic regression (only PANSS total)",ylab = "Logistic regression (all PANSS scores)")
abline(0,1)
png(filename = "Figures/log-all-vs-log-few")
```

```{r}
# AUC
auc.dev = prediction(predict(train.all.glm,combABCD.dev.all,type = "response"), combABCD.dev.all$LeadStatus) %>% performance(measure = "auc") %>% .@y.values

# Cross-entropy
logloss.dev = prediction(predict(train.all.glm,combABCD.dev.all,type = "response"), ifelse(combABCD.dev.all$LeadStatus=="Passed", 1, 0)) %>% performance(measure = "mxe") %>% .@y.values

# ROC curves
prediction(predict(train.all.glm,combABCD.dev.all,type = "response"), combABCD.dev.all$LeadStatus) %>% performance(measure = "tpr", x.measure = "fpr") %>% plot(main = sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) )
```

### Logistic with (all individual scores + lasso)

```{r}
library(glmnet)

# Convert training data to matrix format
x <- model.matrix(LeadStatus~., combABCD.train.all)

# Convert class to numerical variable
y <- ifelse(combABCD.train.all$LeadStatus=="Passed",0,1)

# Do a grid search to identify the best lambda value.
crossv.out <- cv.glmnet(x,y, alpha=1, family="binomial",type.measure="auc")
plot(crossv.out)
```

```{r}
# Min value of lambda
lambda_min <- crossv.out$lambda.min
# Best value of lambda
lambda_1se <- crossv.out$lambda.1se
# Regression coefficients
coef(crossv.out,s=lambda_1se)
```

```{r}
# Get development set data
x_test1 <- model.matrix(LeadStatus~., combABCD.dev.all)

# Predict class, type=”class”
lasso_prob <- predict(crossv.out, newx = x_test1, s=lambda_1se, type="response")

roc.dev = roc(LeadStatus~lasso_prob, data = combABCD.dev.all)
plot(roc.dev,xlim = c(0,1),ylim = c(0,1))

# Kaggle test
test.all$LeadStatus = test.full.glm$LeadStatus
x_test2 <- model.matrix(LeadStatus~., test.all[,-which(names(test.all) == "AssessmentID")])
lasso_prob <- predict(crossv.out, newx = x_test2, s=lambda_1se, type="response")

write.csv(test.all.glm,"Predictions/test.all.glm.csv",row.names = FALSE)

test.all$LeadStatus = lasso_prob
test.lasso = subset(test.all, select = c(AssessmentID,LeadStatus))
write.csv(test.lasso,"Predictions/test.lasso.glm.csv",row.names = FALSE)


# Compare to original logistic regression
plot(test.output.glm$LeadStatus,lasso_prob,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)

summary(crossv.out$glmnet.fit)
```

```{r}
lasso_prob <- predict(crossv.out, newx = x_test1, s=lambda_1se, type="response")

# AUC
auc.dev = prediction(lasso_prob, combABCD.dev.all$LeadStatus) %>%
  performance(measure = "auc") %>%
  .@y.values

# Cross-entropy
logloss.dev = prediction(lasso_prob, ifelse(combABCD.dev.all$LeadStatus=="Passed", 0, 1)) %>%
  performance(measure = "mxe") %>%
  .@y.values

# ROC curves
prediction(lasso_prob, ifelse(combABCD.dev.all$LeadStatus=="Passed", 0, 1)) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot(main = sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) )
```

### LDA

### Training - validation split

Study E has no country of UK, so we take out the country as a predictor.

```{r}
# Provides LDA & QDA model functions
library(MASS)
(LDA.m1 = lda(LeadStatus~., data = combABCD.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")]))
plot(LDA.m1, col="red")
```

I can now use LDA model to make a prediction on the development set

```{r}
dev.LDA.pred = predict(LDA.m1, newdata = combABCD.dev)
table(combABCD.dev$LeadStatus, dev.LDA.pred$class) %>% prop.table() %>% round(4)

# Accuracy rate
mean(dev.LDA.pred$class == combABCD.dev$LeadStatus)

# Error rate
mean(dev.LDA.pred$class != combABCD.dev$LeadStatus)
```

```{r}
# AUC
auc.dev = prediction(dev.LDA.pred$posterior[,2], combABCD.dev$LeadStatus) %>%
  performance(measure = "auc") %>%
  .@y.values

# Cross-entropy
logloss.dev =prediction(dev.LDA.pred$posterior[,2], ifelse(combABCD.dev$LeadStatus=="Passed", 1, 0)) %>%
  performance(measure = "mxe") %>%
  .@y.values

# ROC curves
prediction(dev.LDA.pred$posterior[,2], combABCD.dev$LeadStatus) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot(main = sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) )
```

```{r}
test=E
test.LDA.pred = predict(LDA.m1, newdata = test)
test$LeadStatus = test.LDA.pred$posterior[,1]
test = test[,c("AssessmentID","LeadStatus")]

# Compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)

# Write for kaggle prediction
write.csv(test,"Predictions/LDA-prediction.csv",row.names=FALSE)
```

### Use full training set

```{r}
(LDA.full = lda(LeadStatus~., data = combABCD[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")]))
plot(LDA.full,col="red")
```

```{r}
test=E
test.LDA.pred = predict(LDA.full, newdata = test)
test$LeadStatus = test.LDA.pred$posterior[,1]
test = test[,c("AssessmentID","LeadStatus")]

# Compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)

# Write for Kaggle prediction
write.csv(test,"Predictions/LDA-full-prediction.csv",row.names=FALSE)
```

### Use all predictors

### QDA

```{r}
(QDA.m1 = qda(LeadStatus~., data = combABCD.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")]))
```

We can now use our QDA model to make a prediction on the development set

```{r}
dev.QDA.pred = predict(QDA.m1, newdata = combABCD.dev)
table(combABCD.dev$LeadStatus, dev.QDA.pred$class) %>% prop.table() %>% round(4)

# Accuracy rate
mean(dev.QDA.pred$class == combABCD.dev$LeadStatus)

# Error rate
mean(dev.QDA.pred$class != combABCD.dev$LeadStatus)
```

```{r}
# AUC
auc.dev=prediction(dev.QDA.pred$posterior[,2], combABCD.dev$LeadStatus) %>%
  performance(measure = "auc") %>%
  .@y.values

# Cross-entropy
logloss.dev=prediction(dev.QDA.pred$posterior[,2], ifelse(combABCD.dev$LeadStatus=="Passed", 1, 0)) %>%
  performance(measure = "mxe") %>%
  .@y.values

# ROC curves
prediction(dev.QDA.pred$posterior[,2], combABCD.dev$LeadStatus) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot(main = sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) )
```

```{r}
test=E
test.QDA.pred = predict(QDA.m1, newdata = test)
test$LeadStatus = test.QDA.pred$posterior[,1]
test = test[,c("AssessmentID","LeadStatus")]

# Compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)

# Write for kaggle prediction
write.csv(test,"Predictions/QDA-prediction.csv",row.names=FALSE)
```

### SVM

### Linear

```{r}
# Support Vector Machines methodology
library(e1071)
set.seed(100)

SVM.fit <- svm(LeadStatus~., kernel = "linear",data = combABCD.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], scale=TRUE, probability=TRUE)
summary(SVM.fit)
```

```{r}
plot(SVM.fit,combABCD.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Treatment"), fill=FALSE)
plot(SVM.fit,combABCD.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Control"), fill=FALSE)
```

"X" are the support vectors and directly affect the classification line.

```{r}
dev.svm.pred = predict(SVM.fit,newdata = combABCD.dev,probability=TRUE)
table(combABCD.dev$LeadStatus, dev.svm.pred) %>% prop.table() %>% round(4)

# Accuracy rate
mean(dev.svm.pred == combABCD.dev$LeadStatus)

# Error rate
mean(dev.svm.pred != combABCD.dev$LeadStatus)
```

```{r}
# ROC curves
probabilities = attr(dev.svm.pred, "probabilities")
prediction(probabilities[,2], combABCD.dev$LeadStatus) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot()

# AUC
prediction(probabilities[,2], combABCD.dev$LeadStatus) %>%
  performance(measure = "auc") %>%
  .@y.values

# Cross-entropy
prediction(probabilities[,2], ifelse(combABCD.dev$LeadStatus=="Passed", 1, 0)) %>%
  performance(measure = "mxe") %>%
  .@y.values
```

```{r}
test=E
test.svm.pred = predict(SVM.fit, newdata = test, probability=TRUE)
probabilities = attr(test.svm.pred, "probabilities")
test$LeadStatus = probabilities[,1]
test = test[,c("AssessmentID","LeadStatus")]

# Compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)

# Write for Kaggle prediction
write.csv(test,"Predictions/SVM-prediction.csv",row.names=FALSE)
```

### Radial

```{r}
set.seed(100)
SVM.fit <- svm(LeadStatus~., kernel = "radial",data = combABCD.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], scale=TRUE, probability=TRUE)
summary(SVM.fit)
```

```{r}
plot(SVM.fit,combABCD.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Treatment"), fill=FALSE)
plot(SVM.fit,combABCD.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Control"), fill=FALSE)
```

Points that are represented by an "X" are the support vectors, or the points that directly affect the classification line.

```{r}
dev.svm.pred = predict(SVM.fit,newdata = combABCD.dev,probability=TRUE)
table(combABCD.dev$LeadStatus, dev.svm.pred) %>% prop.table() %>% round(4)

# Accuracy rate
mean(dev.svm.pred == combABCD.dev$LeadStatus)

# Error rate
mean(dev.svm.pred != combABCD.dev$LeadStatus)
```

```{r}
# ROC curves
probabilities = attr(dev.svm.pred, "probabilities")
prediction(probabilities[,2], combABCD.dev$LeadStatus) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot()

# AUC
prediction(probabilities[,2], combABCD.dev$LeadStatus) %>%
  performance(measure = "auc") %>%
  .@y.values

# Cross-entropy
prediction(probabilities[,2], ifelse(combABCD.dev$LeadStatus=="Passed", 1, 0)) %>%
  performance(measure = "mxe") %>%
  .@y.values
```

```{r}
test=E
test.svm.pred = predict(SVM.fit, newdata = test, probability=TRUE)
probabilities = attr(test.svm.pred, "probabilities")
test$LeadStatus = probabilities[,1]
test = test[,c("AssessmentID","LeadStatus")]

# Compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)

# Write for Kaggle prediction
write.csv(test,"Predictions/SVM-radial-prediction.csv",row.names=FALSE)
```

### Polynomial

```{r}
set.seed(100)
SVM.fit <- svm(LeadStatus~., kernel = "poly",degree=2,data = combABCD.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")], scale=TRUE, probability=TRUE)
summary(SVM.fit)
```

```{r}
plot(SVM.fit,combABCD.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Treatment"), fill=FALSE)
plot(SVM.fit,combABCD.train[,c("TxGroup","VisitDay","PANSS_Total","LeadStatus")],VisitDay~PANSS_Total,slice = list(TxGroup = "Control"), fill=FALSE)
```

"X" are the support vectors and directly affect the classification line.

```{r}
dev.svm.pred = predict(SVM.fit,newdata = combABCD.dev,probability=TRUE)
table(combABCD.dev$LeadStatus, dev.svm.pred) %>% prop.table() %>% round(4)

# Accuracy rate
mean(dev.svm.pred == combABCD.dev$LeadStatus)

# Error rate
mean(dev.svm.pred != combABCD.dev$LeadStatus)
```

```{r}
# ROC curves
probabilities = attr(dev.svm.pred, "probabilities")
prediction(probabilities[,2], combABCD.dev$LeadStatus) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot()

# AUC
prediction(probabilities[,2], combABCD.dev$LeadStatus) %>%
  performance(measure = "auc") %>%
  .@y.values

# Cross-entropy
prediction(probabilities[,2], ifelse(combABCD.dev$LeadStatus=="Passed", 1, 0)) %>%
  performance(measure = "mxe") %>%
  .@y.values
```

```{r}
test=E
test.svm.pred = predict(SVM.fit, newdata = test, probability=TRUE)
probabilities = attr(test.svm.pred, "probabilities")
test$LeadStatus = probabilities[,1]
test = test[,c("AssessmentID","LeadStatus")]

# Compare to logistic regression
plot(test.output.glm$LeadStatus,test$LeadStatus,xlim=c(0,0.5),ylim=c(0,0.5))
abline(0,1)

# Write for Kaggle prediction
write.csv(test,"Predictions/SVM-poly2-prediction.csv",row.names=FALSE)
```

### Gradient Boosting Machines

```{r}
h2o.no_progress()
h2o.removeAll()
h2o.init(max_mem_size="8g")
Y.train = "LeadStatus"
X.train = setdiff(names(combABCD.train),c(Y.train,"Country"))
combABCD.train.h2o.GBM = as.h2o(combABCD.train)
combABCD.dev.h2o.GBM = as.h2o(combABCD.dev)
h2o.GBM.fit1 = h2o.gbm(x = X.train,y = Y.train,training_frame = combABCD.train.h2o.GBM, nfolds = 6)
h2o.GBM.fit1

# Define a function to easily plot ROC curve each time
getROC.h2o<-function(h2o.fit,dev.h2o.data){
  pfm.GBM.dev = h2o.performance(h2o.fit,newdata = dev.h2o.data)
logloss.dev = h2o.logloss(pfm.GBM.dev)
auc.dev <- h2o.auc(pfm.GBM.dev)
fpr.dev <- h2o.fpr(pfm.GBM.dev) %>% .[['fpr']]
tpr.dev <- h2o.tpr(pfm.GBM.dev) %>% .[['tpr']]
data.frame(fpr = fpr.dev, tpr = tpr.dev) %>%
  ggplot(aes(fpr, tpr) ) +
  geom_line() +
  ggtitle( sprintf('Development Dataset AUC, LogLoss: %f, %f', auc.dev,logloss.dev) ) +
  theme_minimal()
}

#Plot ROC curve
getROC.h2o(h2o.GBM.fit1,combABCD.dev.h2o.GBM)
ggsave("Figures/GBM_roc.png",device="png",dpi="print")
```

The development logloss from a default GBM model with 6-fold CV is 0.507 - not bad. The default model has 50 trees. We can train for more trees (say up to 500):

```{r}
h2o.GBM.fit2 = h2o.gbm(x=X.train,y=Y.train,training_frame=combABCD.train.h2o.GBM,nfolds=6,ntrees=500,stopping_rounds=10,stopping_tolerance=0,seed=100)

h2o.GBM.fit2
getROC.h2o(h2o.GBM.fit2,combABCD.dev.h2o.GBM)
ggsave("Figures/GBM_more_trees_roc.png",device="png",dpi="print")
```

Now the logloss gets slightly better.

Let's try tuning the parameters: ntrees, max_depth, min_rows, learn_rate, learn_rate_annealing, sample_rate, col_sample_rate. To speed this up, we'll use the development set for validation:

```{r}
hyper_grid=list(
  max_depth = c(3, 4, 5), # 2 < and < 6
  min_rows = c(10, 20, 30, 40),
  learn_rate = c(0.0025, 0.005, 0.01, 0.05), # > 0.001 and < 0.1
  learn_rate_annealing = c(1), # 1 is best
  sample_rate = c(.65, .7,0.75,.8,.85), # > 0.6 and < 0.9
  col_sample_rate = c(0.6, 0.7,.8, .9) # > 0.6 and < 1
)

search_criteria=list(
  strategy="RandomDiscrete",
  stopping_metric="logloss",
  stopping_tolerance=0.005,
  stopping_rounds=10,
  max_runtime_secs=60*1 # report sets runtime to 30 min
)

grid = h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_random_discrete",
  x = X.train,
  y = Y.train,
  training_frame = combABCD.train.h2o.GBM,
  validation_frame = combABCD.dev.h2o.GBM,
  hyper_params = hyper_grid,
  search_criteria = search_criteria,
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 100
  )

grid_perf= h2o.getGrid(grid_id="gbm_random_discrete",sort_by="logloss",decreasing=FALSE)
best_model_id = grid_perf@model_ids[[1]]
best_model.random.discrete = h2o.getModel(best_model_id)
summary(grid_perf)
getROC.h2o(best_model.random.discrete,combABCD.dev.h2o.GBM)
ggsave("Figures/GBM_tuned_roc.png",device="png",dpi="print")
```

```{r}
# Train final model
h2o.final <- h2o.gbm(
  x = X.train,
  y = Y.train,
  training_frame = combABCD.train.h2o.GBM,
  ntrees = 20000,
  learn_rate = 0.01,
  learn_rate_annealing = 1,
  max_depth = 4,
  min_rows = 30,
  sample_rate = 0.75,
  col_sample_rate = 0.9,
  stopping_rounds = 10,
  seed = 100
)

# Model stopped after xx trees
h2o.final@parameters$ntrees
h2o.varimp_plot(h2o.final, num_of_features = 5)
```

Let's predict the Kaggle test set:

```{r}
# Predict values with predict , make a function to use easily every time
getPredict = function(model.h2o,test_df,file.output){
  test.h2o = test_df %>%
  mutate_if(is.factor,factor,ordered = FALSE)%>%
  as.h2o()

  pred = h2o.predict(model.h2o,test.h2o)
  pred_df = as.data.frame(pred)
  test_df$LeadStatus = pred_df$Flagged
  test.output = test_df[,c("AssessmentID","LeadStatus")]
  write.csv(test.output,file.output,row.names = FALSE)

  plot(test.output.glm$LeadStatus,test_df$LeadStatus,xlim = c(0,0.5),ylim = c(0,0.5))
  abline(0,1)
}
# Study E is the test set
test=E
getPredict(best_model.random.discrete,test,"Predictions/test.GBM.csv")
```

### GBM with all predictors

Maybe including all the individual scores would help. First create the h2o dataframes:

```{r}
Y.train = "LeadStatus"
# Predictors
X.train = setdiff(names(combABCD.train.all),c(Y.train,"Country"))
combABCD.train.h2o.GBM = as.h2o(combABCD.train.all)
combABCD.dev.h2o.GBM = as.h2o(combABCD.dev.all)
```

Now I will perform a random discrete grid search again.

```{r}
# Create hyper-parameter grid
hyper_grid <- list(
  max_depth = c(1, 3, 5),
  min_rows = c(1, 5, 10),
  learn_rate = c(0.01, 0.05, 0.1),
  learn_rate_annealing = c(0.99, 1),
  sample_rate = c(0.50, 0.75, 1),
  col_sample_rate = c(0.80, 0.90, 1)
)

search_criteria = list(
  strategy = "RandomDiscrete",
  stopping_metric = "logloss",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 60*1
)

grid = h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid_all",
  x = X.train,
  y = Y.train,
  training_frame = combABCD.train.h2o.GBM,
  validation_frame = combABCD.dev.h2o.GBM,
  hyper_params = hyper_grid,
  search_criteria = search_criteria,
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  seed = 100
  )

grid_perf = h2o.getGrid(grid_id = "gbm_grid_all",sort_by = "logloss",decreasing = FALSE)
best_model_id = grid_perf@model_ids[[1]]
best_model = h2o.getModel(best_model_id)
summary(grid_perf)
getROC.h2o(best_model,combABCD.dev.h2o.GBM)
ggsave("Figures/GBM_all_pred_roc.png",device="png",dpi="print")
```

```{r}
train.h2o <- as.h2o(combABCD.all)

# Train final model with more trees and on full data
h2o.final <- h2o.gbm(
  x = X.train,
  y = Y.train,
  training_frame = train.h2o,
  ntrees = 10000,
  learn_rate = 0.01,
  learn_rate_annealing = 1,
  max_depth = 6,
  min_rows = 30,
  sample_rate = 0.8,
  col_sample_rate = 0.8,
  stopping_rounds = 10,
  seed = 100
)

# Model stopped after xx trees
h2o.final@parameters$ntrees
h2o.varimp_plot(h2o.final)
ggsave("Figures/GBM_var_imp.png",device="png",dpi="print")
```

```{r}
# Remove previous old lead status column
test.all = test.all[,-which(names(test.all) == c("LeadStatus"))] 
getPredict(h2o.final,test.all,"Predictions/test.GBM.all.csv")
```

## Random forests

```{r}
# A faster implementation of randomForest
library(ranger)
set.seed(200)

# Default RF model
m1 <- ranger(
  formula = LeadStatus ~ .,
  data    = combABCD.train.all,
  probability = T
)

m1
```

```{r}
# Prediction
pred_randomForest <- predict(m1, test.all)
plot(test.output.glm$LeadStatus,pred_randomForest$predictions[,1],xlim=c(0,.5),ylim=c(0,.5))
abline(0,1)
```

### Tuning via h2o

```{r}
set.seed(300)

# Hyper-grid
hyper_grid.h2o <- list(
  ntrees      = seq(200, 500, by = 150),
  mtries      = c(15, 20, 25, 30),
  max_depth   = seq(20, 40, by = 5),
  min_rows    = seq(1, 5, by = 2),
  nbins       = seq(10, 30, by = 5),
  sample_rate = c(0.55, 0.632, .75)
)

# Random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "logloss",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 60*1
  )

# Build grid search
random_grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid2",
  x = X.train,
  y = Y.train,
  training_frame = combABCD.train.h2o.GBM,
  hyper_params = hyper_grid.h2o,
  search_criteria = search_criteria
  )

# Collect the results and sort by our model performance metric of choice
grid_perf2 <- h2o.getGrid(
  grid_id = "rf_grid2",
  sort_by = "logloss",
  decreasing = FALSE
  )
```

```{r}
summary(grid_perf2)
```

```{r}
# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf2@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)

# Evaluate the model performance on a test set
h2o.varimp_plot(best_model)
getROC.h2o(best_model,combABCD.dev.h2o.GBM)
ggsave("Figures/rf_roc.png",device="png",dpi="print")

# Train on full data set
train.h2o <- as.h2o(as.data.frame(combABCD.all))
h2o.final <- h2o.randomForest(
  x = X.train,
  y = Y.train,
  training_frame = train.h2o,
  ntrees      = 500,
  mtries      = 15,
  max_depth   = 25,
  min_rows    = 3,
  nbins       = 25,
  sample_rate = 0.75,
  stopping_rounds = 10,
  seed = 100
) 
# Keep these parameters same as those of best model found from hyper-tuning 

# View prediction
test.all = test.all[,-which(names(test.all) == c("LeadStatus"))] 
getPredict(h2o.final,test.all,"Predictions/test.rf.all.csv")
```



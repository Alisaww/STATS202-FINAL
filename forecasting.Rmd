---
title: "Untitled"
author: "Alisaww"
date: "2022-07-29"
output: html_document
---
```{r}
rm(list = ls())
A=read.csv("Study_A.csv")
B=read.csv("Study_B.csv")
C=read.csv("Study_C.csv")
D=read.csv("Study_D.csv")
E=read.csv("Study_E.csv")
length(unique(E$PatientID))
```

```{r}
setwd("/Users/quyunjie/Desktop/STATS202/FinalProjectData")
submission = read.csv("sample_submission_PANSS.csv")
length(submission$PatientID)
length(unique(submission$PatientID))
```

```{r}
A3 = subset(A, select = c(PatientID, Country, TxGroup, Study, VisitDay,PANSS_Total))
B3 = subset(B, select = c(PatientID, Country, TxGroup, Study, VisitDay,PANSS_Total))
C3 = subset(C, select = c(PatientID, Country, TxGroup, Study, VisitDay,PANSS_Total))
D3 = subset(D, select = c(PatientID, Country, TxGroup, Study, VisitDay,PANSS_Total))
E3 = subset(E, select = c(PatientID, Country, TxGroup, Study, VisitDay,PANSS_Total))
combined1 = rbind(A3,B3,C3,D3,E3)
summary(combined1)
```

```{r}
combined1 = combined1[combined1$Study == 'E',]
for (i in 1:dim(combined1)[1]) {
  Number = combined1[i,"PatientID"]
  patient_ID = subset(combined1,PatientID == Number)
  lastday = max(patient_ID$VisitDay)
  #if (lastday==0){ 
    print(combined1[i,])
  #}
  combined1[i,"lastDay"] = lastday }
```

test set
```{r}
select_patients = subset(combined1, VisitDay==lastDay & PatientID %in% submission$PatientID)
dim(select_patients)[1]
```

```{r}
for (Number in unique(select_patients$PatientID)) { # for each unique id
  select1 = subset(select_patients, PatientID==Number)
  if (dim(select1)[1]>1){
    print(select1)
  }
}
```
```{r}
library(h2o)          # a java-based platform
library(plyr)
library(ggplot2)
library(ggsci)
```


```{r}
library(dplyr)
test = distinct(select_patients)
dim(test)[1]

for (Number in unique(test$PatientID)) { # for each unique id
  select1 = subset(test, PatientID==Number)
  if (dim(select1)[1]>1){
    print(select1)
  }
}
test
```
```{r}
install.packages("data.table", type="source")
```

```{r}
library(data.table) 
#all column names except for PANSS_Total
columnnames <- colnames(select_patients)[!grepl('PANSS_Total',colnames(test))]
X <- as.data.table(test)
test = X[,list(mm=mean(PANSS_Total)),columnnames]
names(test)[length(names(test))] = "PANSS_Total"
dim(test)

```
```{r}
MSE = test # for calculating MSE later 
test = subset(test, select = c(PatientID, Country, TxGroup, VisitDay, Study))
test$VisitDay = test$VisitDay + 7 
```






Naive forecasting
```{r}
# create "Naive" submission 
write.csv(select_patients[,c("PatientID","PANSS_Total")],'Predictions_naive-forecast.csv',row.names=FALSE)
```

distribution of scores in study E vs. the other studies. 
```{r}
hist(combined1$PANSS_Total)
hist(E3$PANSS_Total)
hist(select_patients$PANSS_Total)
```
visualize the scores. 
```{r}
selected = subset(E3, PatientID %in% submission$PatientID)
remaining = subset(E3, !(PatientID %in% submission$PatientID))

p = ggplot(NULL,aes())+
  geom_jitter(data=selected,aes(x=VisitDay,y=PANSS_Total,col="Selected Patients"),size=0.5)+
  geom_jitter(data=remaining,aes(x=VisitDay,y=PANSS_Total,col="Rest of Study E"),size=0.5)+
  geom_jitter(data=select_patients,aes(x=VisitDay,y=PANSS_Total,col="last Day"),size=0.5)+
  scale_color_manual(values=rev(pal_aaas("default")(2)))+
  theme_minimal()+
  theme(legend.title=element_blank(),
        #plot.title=element_text(hjust=0.5,size=10,family="Lato"),
        plot.subtitle=element_text(hjust=0.5,size=8,family="Lato"),
        text=element_text(size=10,family="Lato"))
ggsave("plots.png",width=6,height=4,units="in",device="png",dpi="retina")
```

```{r}
# start with MSE set
Naive = subset(MSE, select = c("PatientID","VisitDay","PANSS_Total")) 
# rename column
names(Naive)[2] = "LastVisitDay" 
names(Naive)[3] = "FinalScore" 

# find second to last visit day 
for (Number in Naive$PatientID) { 
  select1 = subset(E3, PatientID==Number)
  x = select1$VisitDay
  n <- length(x)
  if (n==1) {
    Naive[Naive$PatientID == Number,"SecondToLastDay"] = NA
  }else{
    Naive[Naive$PatientID == Number,"SecondToLastDay"] = sort(x,partial=n-1)[n-1]
  }
}

# find second to last score 
Naive$SecondToLastScore = as.numeric(Naive$FinalScore)

for (Number in Naive$PatientID) { 
  day = as.integer(Naive[Naive$PatientID == Number,"SecondToLastDay"])
  if (is.na(day)){
    Naive[Naive$PatientID == Number,"SecondToLastScore"] = NA
  }else{
    select1 = subset(E3, PatientID==Number & VisitDay==day)
    if (dim(select1)[1] > 1){ # take simple average in this case
      Naive[Naive$PatientID == Number,"SecondToLastScore"] = mean(select1$PANSS_Total)
    }else{
      Naive[Naive$PatientID == Number,"SecondToLastScore"] = select1$PANSS_Total
    }
  }
}
```


```{r}
# average final two scores
Naive$FinalScore = as.numeric(Naive$FinalScore)
Naive$PANSS_Total = as.numeric(Naive$FinalScore)
for (Number in Naive$PatientID) { 
  day = as.integer(Naive[Naive$PatientID == Number,"SecondToLastDay"])
  if (is.na(day)){
    Naive[Naive$PatientID == Number,"PANSS_Total"] = Naive[Naive$PatientID == Number,"FinalScore"]
  }else{
    Naive[Naive$PatientID == Number,"PANSS_Total"] = mean(c(as.integer(Naive[Naive$PatientID == Number,"FinalScore"]),as.integer(Naive[Naive$PatientID == Number,"SecondToLastScore"])))
  }
}
# create submission 
write.csv(Naive[,c("PatientID","PANSS_Total")],'less-naive-forecast.csv',row.names=FALSE)
```
Simply averaging the final two days performs quite poorly on the test set. 

We can repeat this process, now storing data for the third day. 
```{r}
# find third to last visit day 
for (Number in Naive$PatientID) {
  select1 = subset(E, PatientID==Number)
  x = select1$VisitDay
  n <- length(x)
  if (n<3) {
    Naive[Naive$PatientID == Number,"ThirdToLastDay"] = NA
  }else{
    Naive[Naive$PatientID == Number,"ThirdToLastDay"] = sort(x,partial=n-2)[n-2]
  }
}
# find third to last score 
Naive$ThirdToLastScore = as.numeric(Naive$FinalScore)
for (Number in Naive$PatientID) {
  day = as.integer(Naive[Naive$PatientID == Number,"ThirdToLastDay"])
  if (is.na(day)){
    Naive[Naive$PatientID == Number,"ThirdToLastScore"] = NA
  }else{
    select1 = subset(E, PatientID==Number & VisitDay==day)
    if (dim(select1)[1] > 1){ # take simple average in this case
      Naive[Naive$PatientID == Number,"ThirdToLastScore"] = mean(select1$PANSS_Total)
    }else{
      Naive[Naive$PatientID == Number,"ThirdToLastScore"] = select1$PANSS_Total
    }
  }
}
```

```{r}

Naive$PANSS_Total = 0*Naive$PANSS_Total
for (Number in Naive$PatientID   ) { 
  day = as.integer(Naive[Naive$PatientID == Number,"SecondToLastDay"])
  if (is.na(day)){
    Naive[Naive$PatientID == Number,"PANSS_Total"] = Naive[Naive$PatientID == Number,"FinalScore"]
  }else{
    day2 = as.integer(Naive[Naive$PatientID == Number,"ThirdToLastDay"])
    if (is.na(day2)){
      alpha = 0.9
      Naive[Naive$PatientID == Number,"PANSS_Total"] = alpha*Naive[Naive$PatientID == Number,"FinalScore"] + alpha*(1-alpha)*Naive[Naive$PatientID == Number,"SecondToLastScore"]
    }else{
      alpha = 0.8
      Naive[Naive$PatientID == Number,"PANSS_Total"] = alpha*Naive[Naive$PatientID == Number,"FinalScore"] + alpha*(1-alpha)*Naive[Naive$PatientID == Number,"SecondToLastScore"]  + alpha*(1-alpha)^2*Naive[Naive$PatientID == Number,"ThirdToLastScore"]
    }
  }
}
# create submission 
write.csv(Naive[,c("PatientID","PANSS_Total")],'less-naive-forecast.csv',row.names=FALSE)
```
Note that we are justified at truncating at three days for $\alpha = 0.8$ since then we incur a $0.8*(0.2)^4*100 = 0.128\%$ error. 

We can also explore just only looking at the two most recent days: 
```{r just-two-days}
Naive$PANSS_Total = 0*Naive$PANSS_Total
for (Number in Naive$PatientID) { # for each unique id
  day = as.integer(Naive[Naive$PatientID == Number,"SecondToLastDay"])
  if (is.na(day)){
    Naive[Naive$PatientID == Number,"PANSS_Total"] = Naive[Naive$PatientID == Number,"FinalScore"]
  }else{
    day2 = as.integer(Naive[Naive$PatientID == Number,"ThirdToLastDay"])
      alpha = 0.9
      Naive[Naive$PatientID == Number,"PANSS_Total"] = alpha*Naive[Naive$PatientID == Number,"FinalScore"] + alpha*(1-alpha)*Naive[Naive$PatientID == Number,"SecondToLastScore"]
  }
}
# create submission script
write.csv(Naive[,c("PatientID","PANSS_Total")],'two-prev-days-forecast.csv',row.names=FALSE)
```
Note that we are justified at truncating at two days for $\alpha = 0.9$ since then we incur a $0.9*(0.1)^3*100 = 0.09\%$ error. 

Finally, what happens if we include the most recent four days? 
```{r find-data-fourth-to-last-day}
# find fourth to last visit day 
for (Number in Naive$PatientID) { # for each unique id
  select1 = subset(E, PatientID==Number)
  x = select1$VisitDay
  n <- length(x)
  if (n<4) {
    Naive[Naive$PatientID == Number,"FourthToLastDay"] = NA
  }else{
    Naive[Naive$PatientID == Number,"FourthToLastDay"] = sort(x,partial=n-3)[n-3]
  }
}
# find fourth to last score 
Naive$FourthToLastScore = as.numeric(Naive$FinalScore)
for (Number in Naive$PatientID) { # for each unique id
  day = as.integer(Naive[Naive$PatientID == Number,"FourthToLastDay"])
  if (is.na(day)){
    Naive[Naive$PatientID == Number,"FourthToLastScore"] = NA
  }else{
   select1 = subset(E, PatientID==Number & VisitDay==day)
    if (dim(select1)[1] > 1){ # take simple average in this case
      Naive[Naive$PatientID == Number,"FourthToLastScore"] = mean(select1$PANSS_Total)
    }else{
      Naive[Naive$PatientID == Number,"FourthToLastScore"] = select1$PANSS_Total
    }
  }
}
```

```{r exp-smooth-4-days}
Naive$PANSS_Total = 0*Naive$PANSS_Total
for (Number in Naive$PatientID) { # for each unique id
  day = as.integer(Naive[Naive$PatientID == Number,"SecondToLastDay"])
  if (is.na(day)){
    Naive[Naive$PatientID == Number,"PANSS_Total"] = Naive[Naive$PatientID == Number,"FinalScore"]
  }else{
    day2 = as.integer(Naive[Naive$PatientID == Number,"ThirdToLastDay"])
    if (is.na(day2)){
      alpha = 0.9
      Naive[Naive$PatientID == Number,"PANSS_Total"] = alpha*Naive[Naive$PatientID == Number,"FinalScore"] + alpha*(1-alpha)*Naive[Naive$PatientID == Number,"SecondToLastScore"]
    }else{
      day3 = as.integer(Naive[Naive$PatientID == Number,"FourthToLastDay"])
      if (is.na(day3)){
      alpha = 0.8
      Naive[Naive$PatientID == Number,"PANSS_Total"] = alpha*Naive[Naive$PatientID == Number,"FinalScore"] + alpha*(1-alpha)*Naive[Naive$PatientID == Number,"SecondToLastScore"]  + alpha*(1-alpha)^2*Naive[Naive$PatientID == Number,"ThirdToLastScore"]
      }else{
      alpha = 0.7
      Naive[Naive$PatientID == Number,"PANSS_Total"] =     alpha*Naive[Naive$PatientID == Number,"FinalScore"] + alpha*(1-alpha)*Naive[Naive$PatientID == Number,"SecondToLastScore"]  + alpha*(1-alpha)^2*Naive[Naive$PatientID == Number,"ThirdToLastScore"] + alpha*(1-alpha)^3*Naive[Naive$PatientID == Number,"FourthToLastScore"]
      }
    }
  }
}
# create submission script
write.csv(Naive[,c("PatientID","PANSS_Total")],'4-day-naive-forecast.csv',row.names=FALSE)
```
Note that we are justified at truncating at two days for $\alpha = 0.7$ since then we incur a $0.7*0.3^4*100 = 0.567\%$ error. 

### Create training set
```{r remove-test-from-total}
dim(combined1)
combine1 = anti_join(combined1, select_patients)
dim(combined1)
```
This removes 410 elements as expected. We should also remove any duplicates from here as we did for the test set. 

```{r remove-training-dups}
training_df = distinct(combined1)
dim(training_df)[1]
``` 
We should also average over cases where all is identical except for the total PANSS score: 
```{r simple-average-training}
keys <- colnames(training_df)[!grepl('PANSS_Total',colnames(training_df))] # all column names except for PANSS_Total
X <- as.data.table(training_df)
training_df = X[,list(mm=mean(PANSS_Total)),keys]
names(training_df)[length(names(training_df))] = "PANSS_Total"
dim(training_df)
training_df = subset(training_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
#training_df = subset(training_df, select = c(PatientID, Country, VisitDay, Study,PANSS_Total))
```

While we could scale some variables, scaling does not matter for decision trees! 
```{r training-scale}
#select_patients_df$PatientID = scale(select_patients_df$PatientID)
#select_patients_df$VisitDay = scale(select_patients_df$VisitDay)
#select_patients_df$PANSS_Total = scale(select_patients_df$PANSS_Total)
```






```{r}
h2o.no_progress()
h2o.init(max_mem_size = "6g")
```


```{r}
training_df = distinct(combined1)
keys <- colnames(training_df)[!grepl('PANSS_Total',colnames(training_df))] # all column names except for PANSS_Total
X <- as.data.table(training_df)
training_df = X[,list(mm=mean(PANSS_Total)),keys]
names(training_df)[length(names(training_df))] = "PANSS_Total"
dim(training_df)
training_df = subset(training_df, select = c(PatientID, Country, TxGroup, VisitDay, Study,PANSS_Total))
y <- "PANSS_Total"
x <- setdiff(names(training_df[,-"PatientID"]), y)

```

```{r}
# turn training set into h2o object
train.h2o <- as.h2o(training_df[,-"PatientID"])
# training basic GBM model with defaults
h2o.fit2 <- h2o.gbm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 10,
  ntrees = 5000,
  stopping_rounds = 10,
  stopping_tolerance = 0,
  max_runtime_secs = 60*10,
  seed = 1
)
# model stopped after xx trees
h2o.fit2@parameters$ntrees
# cross validated MSE
h2o.rmse(h2o.fit2, xval = TRUE)^2
# assess model results
h2o.fit2
```

```{r}
# create training & validation sets
split <- h2o.splitFrame(train.h2o, ratios = 0.75)
train <- split[[1]]
valid <- split[[2]]
# create hyperparameter grid
rm(hyper_grid)
hyper_grid = list(
  max_depth = c(1,3,4), # depth of each tree
  min_rows = c(5,10,20), # minimum observations in a terminal node
  learn_rate = c(0.005, 0.01, 0.05, 0.1),
  learn_rate_annealing = c(1), # 1 tends to always beat 0.99
  sample_rate = c(.65, 0.7, 0.75, 0.8), # row sample rate. better to have less than 1 it seems
  col_sample_rate = c(0.7, .8, .9) # always better to have less than 1 here
)
# number of combinations
nrow(expand.grid(hyper_grid))
# # perform grid search
# grid <- h2o.grid(
#   algorithm = "gbm",
#   grid_id = "gbm_grid1",
#   x = x,
#   y = y,
#   training_frame = train,
#   validation_frame = valid,
#   hyper_params = hyper_grid,
#   ntrees = 10000,
#   stopping_rounds = 10,
#   #stopping_tolerance = 0,
#   seed = 1
#   )
#
# # collect the results and sort by our model performance metric of choice
# grid_perf <- h2o.getGrid(
#   grid_id = "gbm_grid1",
#   sort_by = "mse",
#   decreasing = FALSE
#   )
# grid_perf
```

```{r}
# random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.005, # MSE tolerance
  stopping_rounds = 10,   # stop if 10 consecutive trees have no improvement
  max_runtime_secs = 60*1 # limit how long it runs when debugging 
  )
# perform grid search
gbm_grid2 <- h2o.grid(
  algorithm = "gbm",
  grid_id = "gbm_grid2",
  x = x,
  y = y,
  training_frame = train,
  validation_frame = valid,
  hyper_params = hyper_grid,
  search_criteria = search_criteria, # add search criteria
  ntrees = 10000,
  #stopping_rounds = 10, # stop if none of the last 10 models managed to have a 0.5% improvement in MSE compared to best model before that
  #stopping_tolerance = 0,
  seed = 1
  )
# collect the results and sort by our model performance metric of choice
grid_perf <- h2o.getGrid(
  grid_id = "gbm_grid2",
  sort_by = "mse",
  decreasing = FALSE
  )
grid_perf
```

```{r}
# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
# Now let’s get performance metrics on the best model
h2o.performance(model = best_model, valid = TRUE)
```

```{r}
# train final model
h2o.final <- h2o.gbm(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 10,
  ntrees = 10000,
  learn_rate = 0.1,
  learn_rate_annealing = 1,
  max_depth = 4,
  min_rows = 10,
  sample_rate = 0.75,
  col_sample_rate = 0.9,
  stopping_rounds = 10,
  #stopping_tolerance = 0.005,
  seed = 1
)
# model stopped after xx trees
h2o.final@parameters$ntrees
# cross validated MSE
h2o.rmse(h2o.final, xval = TRUE)^2
```

```{r}
#visualization
h2o.varimp_plot(h2o.final, num_of_features = 5)
```

```{r}
# convert test set to h2o object
test.h2o <- as.h2o(select_patients)
# evaluate performance on new data
h2o.performance(model = h2o.final, newdata = test.h2o)
# predict values with predict
h2o.predict(h2o.final, newdata = test.h2o) # predict with h2o.predict
test.h2o$prediction = predict(h2o.final, test.h2o) # gives same result as above
# write to csv for Kaggle submission
forecast.h2o <- as.h2o(test)
forecast.h2o$PANSS_Total = predict(h2o.final, forecast.h2o)
h2o.exportFile(forecast.h2o[,c("PatientID","PANSS_Total")],'h2o-prediction.csv',force=TRUE)
```

```{r}
install.packages("randomForest")
```

random forests
```{r}
library(randomForest) # basic implementation
# for reproduciblity
set.seed(1)
# default RF model
m1 <- randomForest(
  formula = PANSS_Total ~ .,
  data    = training_df[,-"PatientID"],
  mtry = 2
)
m1
plot(m1)
```

```{r}
# number of trees with lowest MSE
which.min(m1$mse)
# MSE of this optimal random forest
m1$mse[which.min(m1$mse)]
```

```{r}
require(tidyr)
require(dplyr)
# create training and validation data
set.seed(1)
# split data
training_rows = sample(1:nrow(training_df), floor(nrow(training_df)*0.8))
train_v2 = training_df[training_rows,]
valid = training_df[-training_rows,]
x_test = valid
y_test = valid$PANSS_Total
rf_oob_comp <- randomForest(
  formula = PANSS_Total ~ .,
  data    = train_v2[,-"PatientID"],
  xtest   = x_test[,-c("PatientID","PANSS_Total")],
  ytest   = y_test
)
# extract OOB & validation errors
oob <- rf_oob_comp$mse
validation <- rf_oob_comp$test$mse
# compare error rates
tibble::tibble(
  `Out of Bag Error` = oob,
  `Test error` = validation,
  ntrees = 1:rf_oob_comp$ntree
) %>%
  gather(Metric, MSE, -ntrees) %>%
  ggplot(aes(ntrees, MSE, color = Metric)) +
  geom_line() +
  xlab("Number of trees")
```

```{r}
update.packages("data.table")
```

```{r}
install.packages('bit64', repos = "https://cran.rstudio.com")
```


```{r}
# start up h2o
h2o.init(max_mem_size = "6g")
set.seed(1)
# create feature names
y <- "PANSS_Total"
x <- setdiff(names(training_df[,-"PatientID"]), y)
# turn training set into h2o object
train.h2o <- as.h2o(training_df[,-"PatientID"])
# only train on study E
#x <- setdiff(names(training_df[,-c("PatientID","Study")]), y)
#train.h2o <- as.h2o(subset(training_df, Study=="E",select=c(Country, TxGroup, VisitDay, PANSS_Total)))
# second hypergrid
hyper_grid.h2o <- list(
  ntrees      = seq(300, 550, by = 50),
  mtries      = 2,
  max_depth   = seq(15, 45, by = 5),
  min_rows    = seq(7, 11, by = 1),
  nbins       = seq(5, 25, by = 5),
  sample_rate = c(0.4,0.45,0.5,0.55,.6,.65,.7)
)
# random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 60*1 # run for a short time we debugging script 
  )
# build grid search
random_grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid2",
  x = x,
  y = y,
  training_frame = train.h2o,
  hyper_params = hyper_grid.h2o,
  search_criteria = search_criteria
  )
# collect the results and sort by our model performance metric of choice
grid_perf2 <- h2o.getGrid(
  grid_id = "rf_grid2",
  sort_by = "mse",
  decreasing = FALSE
  )
print(grid_perf2)
```
```{r}
grid_perf2 <- h2o.getGrid(
  grid_id = "rf_grid2",
  sort_by = "mse",
  decreasing = FALSE
  )
print(grid_perf2)
```


```{r}
# first grid
hyper_grid.h2o <- list(
  ntrees      = seq(200, 500, by = 150), # best all had 350 min so set 350 as new min
  mtries      = seq(2,4, by = 1), # best all have 2 so set this identically to 2
  max_depth   = seq(20, 40, by = 5),
  min_rows    = seq(1, 5, by = 2), # best all have 5 (so set 5 as min)
  nbins       = seq(10, 30, by = 5),
  sample_rate = c(.55, .632, .75) # best all have 0.55 so vary around this
) # best model from this one has test MSE of 131.2987
# second hypergrid
hyper_grid.h2o <- list(
  ntrees      = seq(350, 500, by = 75), # none of the top 5 use 500
  mtries      = 2,
  max_depth   = seq(20, 40, by = 5), # none of top 5 use 40
  min_rows    = seq(5, 10, by = 2), # none of the top 5 models use 5
  nbins       = seq(10, 30, by = 5), # none of top 5 use 10
  sample_rate = c(0.45,.55, .65) # none of the top 5 models use sample_rate of 0.65
)
```

```{r}
# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf2@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
h2o.varimp_plot(best_model)
# Now let’s evaluate the model performance on a test set
select_patients_df.h2o <- as.h2o(select_patients_df)
best_model_perf <- h2o.performance(model = best_model, newdata = select_patients_df.h2o)
# View prediction
prediction = predict(best_model, select_patients_df.h2o)
plot(as.vector(prediction), select_patients_df$PANSS_Total,xlim=c(30,100), ylim=c(30,100))
abline(0,1) # line with y-intercept 0 and slope 1
# RMSE of best model
h2o.mse(best_model_perf)
# write to csv for Kaggle submission
forecast.h2o <- as.h2o(test_df)
forecast.h2o$PANSS_Total = predict(best_model, forecast.h2o)
h2o.exportFile(forecast.h2o[,c("PatientID","PANSS_Total")],'rf-prediction.csv',force=TRUE)
```
## Linear models
```{r adjust-dataframes}
#training_df = subset(training_df, Study=="E", select = c(PatientID, TxGroup, VisitDay, Study, PANSS_Total))
training_df = subset(training_df, select = c(PatientID, TxGroup, VisitDay, PANSS_Total))
MSE= subset(MSE, Study=="E", select = c(PatientID, TxGroup, VisitDay, Study, PANSS_Total))
NSE= subset(MSE, select = c(PatientID, TxGroup, VisitDay, PANSS_Total))
```

### Linear regression
Fit a linear model using least squares on the training set, and report the test error obtained.
```{r linear}
linear.mod = lm(PANSS_Total ~., data=training_df)
summary(linear.mod)
# Calculate test MSE
mean((dev_df$PANSS_Total - predict(linear.mod, dev_df))^2)
```

### Ridge regression
Fit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained.
```{r ridge}
library(glmnet)
set.seed(1)
# Create design matrices
train.mat = model.matrix(PANSS_Total ~ .-PatientID, data = training_df)
test.mat = model.matrix(PANSS_Total ~ .-PatientID, data = MSE)
# adding exp(-VisitDay) didn't seem to help much
# Ridge regression for array of lambda values
#grid=10^seq(10,-3,length=100)
#ridge.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=0,lambda=grid, thresh=1e-12)
ridge.mod=glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge.mod, xvar = "lambda")
# Find lambda through cross-validation
cv.out = cv.glmnet(train.mat, training_df$PANSS_Total, alpha = 0)
ridge.cv.out = cv.out
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
# Calculate test MSE
ridge.pred=predict(ridge.mod,s=bestlam,newx=test.mat)
mean((ridge.pred - dev_df$PANSS_Total)^2)
```
Note that the first and second vertical dashed lines represent the λ value with the minimum MSE and the largest λ value within one standard error of the minimum MSE.

```{r ridge-lambda}
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
# visualize how much we can restrain coefficients while still having predictive accuracy
ridge_min <- glmnet(x = train.mat, y = training_df$PANSS_Total, alpha=0)
plot(ridge_min, xvar = "lambda")
abline(v = log(cv.out$lambda.1se), col = "red", lty = "dashed")
library(broom)
coef(cv.out, s = "lambda.1se") %>%
  tidy() %>%
  filter(row != "(Intercept)") %>%
  top_n(25, wt = abs(value)) %>%
  ggplot(aes(value, reorder(row, value))) +
  geom_point() +
  ggtitle("Top 25 influential variables") +
  xlab("Coefficient") +
  ylab(NULL)
```

### Lasso regression
Fit a lasso model on the training set, with λ chosen by crossvalidation
```{r lasso}
library(glmnet)
set.seed(1)
# Lasso regression for array of lambda values
#grid=10^seq(10,-3,length=100)
#lasso.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1,lambda=grid, thresh=1e-12)
lasso.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1)
plot(lasso.mod, xvar = "lambda")
# Find lambda through cross-validation
cv.out=cv.glmnet(train.mat, training_df$PANSS_Total,alpha=1)
lasso.cv.out = cv.out
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
# Calculate test MSE
lasso.pred=predict(lasso.mod,s=bestlam,newx=test.mat)
mean((lasso.pred-dev_df$PANSS_Total)^2)
predict(cv.out, s = bestlam, type = "coefficients")
```

```{r lasso-error}
min(cv.out$cvm)       # minimum MSE
cv.out$lambda.min     # lambda for this min MSE
cv.out$cvm[cv.out$lambda == cv.out$lambda.1se]  # 1 st.error of min MSE
cv.out$lambda.1se  # lambda for this MSE
# visualize lasso results
lasso.mod=glmnet(train.mat, training_df$PANSS_Total,alpha=1)
plot(lasso.mod, xvar = "lambda")
abline(v = log(cv.out$lambda.min), col = "red", lty = "dashed")
abline(v = log(cv.out$lambda.1se), col = "red", lty = "dashed")
# most influential variables
coef(cv.out, s = "lambda.1se") %>%
  tidy() %>%
  filter(row != "(Intercept)") %>%
  ggplot(aes(value, reorder(row, value), color = value > 0)) +
  geom_point(show.legend = FALSE) +
  ggtitle("Influential variables") +
  xlab("Coefficient") +
  ylab(NULL)
```

```{r ridge-vs-lasso-error}
# minimum Ridge MSE
min(ridge.cv.out$cvm)
plot(ridge.pred,dev_df$PANSS_Total,xlim=c(20,100), ylim=c(20,100))
# minimum Lasso MSE
min(lasso.cv.out$cvm)
plot(lasso.pred,dev_df$PANSS_Total,xlim=c(20,100), ylim=c(20,100))
```

## Multivariate Adaptive Regression Spline (MARS)
```{r basic-mars}
library(earth)     # fit MARS models
# Fit a basic MARS model
mars1 <- earth(
  PANSS_Total ~ .,
  data = training_df[,-"PatientID"]
)
# Print model summary
print(mars1)
summary(mars1) %>% .$coefficients %>% head(10)
plot(mars1, which = 1)
```

```{r mars-interactions}
# Fit a basic MARS model
mars2 <- earth(
  PANSS_Total ~ .,
  data = training_df[,-"PatientID"],
  degree = 3
)
# check out the first 10 coefficient terms
print(mars2)
summary(mars2) %>% .$coefficients %>% head(10)
plot(mars2, which = 1)
```

### Tuning
```{r mars-grid}
# create a tuning grid
hyper_grid <- expand.grid(
  degree = 1:3,
  nprune = seq(1, 16, by = 2)
  )
head(hyper_grid)
```

```{r mars-tune}
library(caret)
set.seed(1)
# cross validated model
tuned_mars <- train(
  x = subset(training_df[,-"PatientID"], select = -PANSS_Total),
  y = training_df$PANSS_Total,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid
)
# best model
tuned_mars$bestTune
summary(tuned_mars)
# plot results
ggplot(tuned_mars)
```
See http://uc-r.github.io/mars for how to visualize/interpret results further.

### Visualize
```{r mars-importance}
library(vip)       # variable importance
# variable importance plots
p1 <- vip(tuned_mars, num_features = 40, bar = FALSE, value = "gcv") + ggtitle("GCV")
p2 <- vip(tuned_mars, num_features = 40, bar = FALSE, value = "rss") + ggtitle("RSS")
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

```{r test-mse}
test_predict = predict(tuned_mars, dev_df)
mean((dev_df$PANSS_Total - test_predict)^2)
plot(as.vector(test_predict), dev_df$PANSS_Total,xlim=c(30,100), ylim=c(30,100))
abline(0,1)
test_df$PANSS_Total = predict(tuned_mars, test_df)
write.csv(test_df[,c("PatientID","PANSS_Total")],'mars-forecast.csv',row.names=FALSE)
```


```{r}

```

```{r}

```

```{r}

```

```{r}

```

